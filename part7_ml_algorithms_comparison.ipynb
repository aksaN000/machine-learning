{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df295fbe",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms: Comprehensive Comparison and Analysis\n",
    "\n",
    "## A Complete Guide to Classification and Regression Algorithms\n",
    "\n",
    "This notebook provides an in-depth comparison of popular machine learning algorithms across different types of problems. We'll analyze their performance, strengths, weaknesses, and use cases.\n",
    "\n",
    "### Covered Algorithms:\n",
    "**Classification:**\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Support Vector Machines\n",
    "- Naive Bayes\n",
    "- K-Nearest Neighbors\n",
    "- Gradient Boosting\n",
    "- Neural Networks\n",
    "\n",
    "**Regression:**\n",
    "- Linear Regression\n",
    "- Polynomial Regression\n",
    "- Ridge/Lasso Regression\n",
    "- Decision Tree Regression\n",
    "- Random Forest Regression\n",
    "- SVR\n",
    "- Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0277884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, validation_curve\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc0ede",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "\n",
    "We'll use multiple datasets to comprehensively evaluate algorithm performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9612a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare datasets\n",
    "def load_datasets():\n",
    "    \"\"\"\n",
    "    Load various datasets for comprehensive algorithm testing.\n",
    "    \"\"\"\n",
    "    datasets_dict = {}\n",
    "    \n",
    "    # 1. Iris Dataset (Multi-class classification)\n",
    "    iris = datasets.load_iris()\n",
    "    datasets_dict['iris'] = {\n",
    "        'X': iris.data,\n",
    "        'y': iris.target,\n",
    "        'type': 'classification',\n",
    "        'description': 'Iris flower species classification (3 classes)',\n",
    "        'features': iris.feature_names,\n",
    "        'targets': iris.target_names\n",
    "    }\n",
    "    \n",
    "    # 2. Wine Dataset (Multi-class classification)\n",
    "    wine = datasets.load_wine()\n",
    "    datasets_dict['wine'] = {\n",
    "        'X': wine.data,\n",
    "        'y': wine.target,\n",
    "        'type': 'classification',\n",
    "        'description': 'Wine classification (3 classes)',\n",
    "        'features': wine.feature_names,\n",
    "        'targets': wine.target_names\n",
    "    }\n",
    "    \n",
    "    # 3. Breast Cancer Dataset (Binary classification)\n",
    "    cancer = datasets.load_breast_cancer()\n",
    "    datasets_dict['breast_cancer'] = {\n",
    "        'X': cancer.data,\n",
    "        'y': cancer.target,\n",
    "        'type': 'classification',\n",
    "        'description': 'Breast cancer diagnosis (2 classes)',\n",
    "        'features': cancer.feature_names,\n",
    "        'targets': cancer.target_names\n",
    "    }\n",
    "    \n",
    "    # 4. Boston Housing Dataset (Regression)\n",
    "    # Create synthetic housing data similar to Boston housing\n",
    "    np.random.seed(42)\n",
    "    n_samples = 506\n",
    "    n_features = 13\n",
    "    \n",
    "    X_housing = np.random.randn(n_samples, n_features)\n",
    "    # Create realistic housing prices\n",
    "    y_housing = (\n",
    "        X_housing[:, 0] * 5 +  # Room number effect\n",
    "        X_housing[:, 1] * -3 +  # Crime rate effect (negative)\n",
    "        X_housing[:, 2] * 2 +   # Accessibility effect\n",
    "        np.random.normal(0, 2, n_samples) + 25  # Base price + noise\n",
    "    )\n",
    "    y_housing = np.maximum(y_housing, 5)  # Minimum price\n",
    "    \n",
    "    datasets_dict['housing'] = {\n",
    "        'X': X_housing,\n",
    "        'y': y_housing,\n",
    "        'type': 'regression',\n",
    "        'description': 'Housing price prediction',\n",
    "        'features': [f'feature_{i}' for i in range(n_features)],\n",
    "        'targets': None\n",
    "    }\n",
    "    \n",
    "    # 5. Diabetes Dataset (Regression)\n",
    "    diabetes = datasets.load_diabetes()\n",
    "    datasets_dict['diabetes'] = {\n",
    "        'X': diabetes.data,\n",
    "        'y': diabetes.target,\n",
    "        'type': 'regression',\n",
    "        'description': 'Diabetes progression prediction',\n",
    "        'features': diabetes.feature_names,\n",
    "        'targets': None\n",
    "    }\n",
    "    \n",
    "    return datasets_dict\n",
    "\n",
    "# Load all datasets\n",
    "datasets_dict = load_datasets()\n",
    "\n",
    "print(\"Available datasets:\")\n",
    "print(\"=\" * 50)\n",
    "for name, data in datasets_dict.items():\n",
    "    print(f\"{name}: {data['description']}\")\n",
    "    print(f\"  - Samples: {data['X'].shape[0]}, Features: {data['X'].shape[1]}\")\n",
    "    print(f\"  - Type: {data['type']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3fe94",
   "metadata": {},
   "source": [
    "## 2. Algorithm Definitions and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define algorithms for classification\n",
    "classification_algorithms = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
    "    'SVM (Linear)': SVC(kernel='linear', random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'K-NN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "    'K-NN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Define algorithms for regression\n",
    "regression_algorithms = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=1.0),\n",
    "    'Elastic Net': ElasticNet(alpha=1.0, l1_ratio=0.5),\n",
    "    'Polynomial (degree=2)': Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        ('linear', LinearRegression())\n",
    "    ]),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR (RBF)': SVR(kernel='rbf'),\n",
    "    'SVR (Linear)': SVR(kernel='linear'),\n",
    "    'K-NN Regression': KNeighborsRegressor(n_neighbors=5),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'Neural Network': MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "print(f\"Classification algorithms: {len(classification_algorithms)}\")\n",
    "print(f\"Regression algorithms: {len(regression_algorithms)}\")\n",
    "print(\"\\nClassification algorithms:\")\n",
    "for i, name in enumerate(classification_algorithms.keys(), 1):\n",
    "    print(f\"{i:2d}. {name}\")\n",
    "\n",
    "print(\"\\nRegression algorithms:\")\n",
    "for i, name in enumerate(regression_algorithms.keys(), 1):\n",
    "    print(f\"{i:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17692c11",
   "metadata": {},
   "source": [
    "## 3. Comprehensive Algorithm Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_algorithms(X, y, algorithms, dataset_name):\n",
    "    \"\"\"\n",
    "    Evaluate classification algorithms on a dataset.\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, algorithm in algorithms.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use scaled data for algorithms that need it\n",
    "        if name in ['Logistic Regression', 'SVM (RBF)', 'SVM (Linear)', 'K-NN (k=5)', 'K-NN (k=3)', 'Neural Network']:\n",
    "            algorithm.fit(X_train_scaled, y_train)\n",
    "            y_pred = algorithm.predict(X_test_scaled)\n",
    "            cv_scores = cross_val_score(algorithm, X_train_scaled, y_train, cv=5)\n",
    "        else:\n",
    "            algorithm.fit(X_train, y_train)\n",
    "            y_pred = algorithm.predict(X_test)\n",
    "            cv_scores = cross_val_score(algorithm, X_train, y_train, cv=5)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        results.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Algorithm': name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'CV Mean': cv_scores.mean(),\n",
    "            'CV Std': cv_scores.std(),\n",
    "            'Training Time (s)': end_time - start_time\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def evaluate_regression_algorithms(X, y, algorithms, dataset_name):\n",
    "    \"\"\"\n",
    "    Evaluate regression algorithms on a dataset.\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, algorithm in algorithms.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use scaled data for algorithms that need it\n",
    "        if name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'Elastic Net', \n",
    "                   'SVR (RBF)', 'SVR (Linear)', 'K-NN Regression', 'Neural Network']:\n",
    "            algorithm.fit(X_train_scaled, y_train)\n",
    "            y_pred = algorithm.predict(X_test_scaled)\n",
    "            cv_scores = cross_val_score(algorithm, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "        else:\n",
    "            algorithm.fit(X_train, y_train)\n",
    "            y_pred = algorithm.predict(X_test)\n",
    "            cv_scores = cross_val_score(algorithm, X_train, y_train, cv=5, scoring='r2')\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Algorithm': name,\n",
    "            'R² Score': r2,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'CV R² Mean': cv_scores.mean(),\n",
    "            'CV R² Std': cv_scores.std(),\n",
    "            'Training Time (s)': end_time - start_time\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluations\n",
    "print(\"Running comprehensive algorithm evaluation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_classification_results = []\n",
    "all_regression_results = []\n",
    "\n",
    "for dataset_name, dataset_info in datasets_dict.items():\n",
    "    print(f\"Evaluating on {dataset_name} dataset...\")\n",
    "    \n",
    "    if dataset_info['type'] == 'classification':\n",
    "        results = evaluate_classification_algorithms(\n",
    "            dataset_info['X'], dataset_info['y'], \n",
    "            classification_algorithms, dataset_name\n",
    "        )\n",
    "        all_classification_results.append(results)\n",
    "    \n",
    "    elif dataset_info['type'] == 'regression':\n",
    "        results = evaluate_regression_algorithms(\n",
    "            dataset_info['X'], dataset_info['y'], \n",
    "            regression_algorithms, dataset_name\n",
    "        )\n",
    "        all_regression_results.append(results)\n",
    "\n",
    "# Combine all results\n",
    "if all_classification_results:\n",
    "    classification_df = pd.concat(all_classification_results, ignore_index=True)\n",
    "if all_regression_results:\n",
    "    regression_df = pd.concat(all_regression_results, ignore_index=True)\n",
    "\n",
    "print(\"\\nEvaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6438bf",
   "metadata": {},
   "source": [
    "## 4. Classification Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc01123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification results\n",
    "print(\"CLASSIFICATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Overall performance summary\n",
    "classification_summary = classification_df.groupby('Algorithm').agg({\n",
    "    'Accuracy': ['mean', 'std'],\n",
    "    'F1-Score': ['mean', 'std'],\n",
    "    'Training Time (s)': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "classification_summary.columns = ['Acc_Mean', 'Acc_Std', 'F1_Mean', 'F1_Std', 'Time_Mean', 'Time_Std']\n",
    "classification_summary = classification_summary.sort_values('Acc_Mean', ascending=False)\n",
    "\n",
    "print(\"\\nOverall Classification Performance (averaged across datasets):\")\n",
    "print(classification_summary)\n",
    "\n",
    "# Detailed results by dataset\n",
    "print(\"\\n\\nDetailed Results by Dataset:\")\n",
    "print(\"=\" * 50)\n",
    "for dataset in classification_df['Dataset'].unique():\n",
    "    dataset_results = classification_df[classification_df['Dataset'] == dataset]\n",
    "    dataset_results = dataset_results.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{dataset.upper()} Dataset:\")\n",
    "    print(dataset_results[['Algorithm', 'Accuracy', 'F1-Score', 'CV Mean', 'Training Time (s)']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92028413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "accuracy_pivot = classification_df.pivot(index='Algorithm', columns='Dataset', values='Accuracy')\n",
    "sns.heatmap(accuracy_pivot, annot=True, cmap='RdYlGn', fmt='.3f', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Accuracy Scores by Algorithm and Dataset')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. F1-Score comparison\n",
    "f1_pivot = classification_df.pivot(index='Algorithm', columns='Dataset', values='F1-Score')\n",
    "sns.heatmap(f1_pivot, annot=True, cmap='RdYlGn', fmt='.3f', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('F1-Scores by Algorithm and Dataset')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Training time comparison\n",
    "time_data = classification_df.groupby('Algorithm')['Training Time (s)'].mean().sort_values()\n",
    "sns.barplot(x=time_data.values, y=time_data.index, ax=axes[1, 0], palette='viridis')\n",
    "axes[1, 0].set_title('Average Training Time by Algorithm')\n",
    "axes[1, 0].set_xlabel('Training Time (seconds)')\n",
    "\n",
    "# 4. Overall performance ranking\n",
    "overall_score = classification_df.groupby('Algorithm')['Accuracy'].mean().sort_values(ascending=False)\n",
    "sns.barplot(x=overall_score.values, y=overall_score.index, ax=axes[1, 1], palette='coolwarm')\n",
    "axes[1, 1].set_title('Overall Classification Performance Ranking')\n",
    "axes[1, 1].set_xlabel('Average Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a382b",
   "metadata": {},
   "source": [
    "## 5. Regression Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83940ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display regression results\n",
    "print(\"REGRESSION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Overall performance summary\n",
    "regression_summary = regression_df.groupby('Algorithm').agg({\n",
    "    'R² Score': ['mean', 'std'],\n",
    "    'RMSE': ['mean', 'std'],\n",
    "    'Training Time (s)': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "regression_summary.columns = ['R2_Mean', 'R2_Std', 'RMSE_Mean', 'RMSE_Std', 'Time_Mean', 'Time_Std']\n",
    "regression_summary = regression_summary.sort_values('R2_Mean', ascending=False)\n",
    "\n",
    "print(\"\\nOverall Regression Performance (averaged across datasets):\")\n",
    "print(regression_summary)\n",
    "\n",
    "# Detailed results by dataset\n",
    "print(\"\\n\\nDetailed Results by Dataset:\")\n",
    "print(\"=\" * 50)\n",
    "for dataset in regression_df['Dataset'].unique():\n",
    "    dataset_results = regression_df[regression_df['Dataset'] == dataset]\n",
    "    dataset_results = dataset_results.sort_values('R² Score', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{dataset.upper()} Dataset:\")\n",
    "    print(dataset_results[['Algorithm', 'R² Score', 'RMSE', 'CV R² Mean', 'Training Time (s)']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a338f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. R² Score comparison\n",
    "r2_pivot = regression_df.pivot(index='Algorithm', columns='Dataset', values='R² Score')\n",
    "sns.heatmap(r2_pivot, annot=True, cmap='RdYlGn', fmt='.3f', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('R² Scores by Algorithm and Dataset')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. RMSE comparison (lower is better)\n",
    "rmse_pivot = regression_df.pivot(index='Algorithm', columns='Dataset', values='RMSE')\n",
    "sns.heatmap(rmse_pivot, annot=True, cmap='RdYlGn_r', fmt='.2f', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('RMSE by Algorithm and Dataset (Lower is Better)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Training time comparison\n",
    "time_data = regression_df.groupby('Algorithm')['Training Time (s)'].mean().sort_values()\n",
    "sns.barplot(x=time_data.values, y=time_data.index, ax=axes[1, 0], palette='viridis')\n",
    "axes[1, 0].set_title('Average Training Time by Algorithm')\n",
    "axes[1, 0].set_xlabel('Training Time (seconds)')\n",
    "\n",
    "# 4. Overall performance ranking\n",
    "overall_score = regression_df.groupby('Algorithm')['R² Score'].mean().sort_values(ascending=False)\n",
    "sns.barplot(x=overall_score.values, y=overall_score.index, ax=axes[1, 1], palette='coolwarm')\n",
    "axes[1, 1].set_title('Overall Regression Performance Ranking')\n",
    "axes[1, 1].set_xlabel('Average R² Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fdc55c",
   "metadata": {},
   "source": [
    "## 6. Algorithm Strengths and Weaknesses Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fbef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive algorithm analysis\n",
    "def create_algorithm_profile():\n",
    "    \"\"\"\n",
    "    Create detailed profiles for each algorithm.\n",
    "    \"\"\"\n",
    "    algorithm_profiles = {\n",
    "        'Logistic Regression': {\n",
    "            'Type': 'Linear',\n",
    "            'Complexity': 'Low',\n",
    "            'Interpretability': 'High',\n",
    "            'Scalability': 'High',\n",
    "            'Overfitting Risk': 'Low',\n",
    "            'Best For': 'Linear separable data, baseline model',\n",
    "            'Strengths': 'Fast, interpretable, probabilistic output',\n",
    "            'Weaknesses': 'Assumes linear relationship, sensitive to outliers'\n",
    "        },\n",
    "        'Decision Tree': {\n",
    "            'Type': 'Tree-based',\n",
    "            'Complexity': 'Medium',\n",
    "            'Interpretability': 'High',\n",
    "            'Scalability': 'Medium',\n",
    "            'Overfitting Risk': 'High',\n",
    "            'Best For': 'Non-linear data, feature selection',\n",
    "            'Strengths': 'Interpretable, handles non-linear relationships',\n",
    "            'Weaknesses': 'Prone to overfitting, unstable'\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'Type': 'Ensemble',\n",
    "            'Complexity': 'Medium-High',\n",
    "            'Interpretability': 'Medium',\n",
    "            'Scalability': 'Medium',\n",
    "            'Overfitting Risk': 'Low',\n",
    "            'Best For': 'General purpose, feature importance',\n",
    "            'Strengths': 'Robust, handles missing values, feature importance',\n",
    "            'Weaknesses': 'Less interpretable, can overfit with noise'\n",
    "        },\n",
    "        'SVM (RBF)': {\n",
    "            'Type': 'Kernel-based',\n",
    "            'Complexity': 'High',\n",
    "            'Interpretability': 'Low',\n",
    "            'Scalability': 'Low',\n",
    "            'Overfitting Risk': 'Medium',\n",
    "            'Best For': 'High-dimensional data, non-linear patterns',\n",
    "            'Strengths': 'Effective in high dimensions, memory efficient',\n",
    "            'Weaknesses': 'Slow on large datasets, requires scaling'\n",
    "        },\n",
    "        'Naive Bayes': {\n",
    "            'Type': 'Probabilistic',\n",
    "            'Complexity': 'Low',\n",
    "            'Interpretability': 'Medium',\n",
    "            'Scalability': 'High',\n",
    "            'Overfitting Risk': 'Low',\n",
    "            'Best For': 'Text classification, small datasets',\n",
    "            'Strengths': 'Fast, works with small data, handles missing values',\n",
    "            'Weaknesses': 'Strong independence assumption, categorical inputs'\n",
    "        },\n",
    "        'K-NN (k=5)': {\n",
    "            'Type': 'Instance-based',\n",
    "            'Complexity': 'Low',\n",
    "            'Interpretability': 'High',\n",
    "            'Scalability': 'Low',\n",
    "            'Overfitting Risk': 'Medium',\n",
    "            'Best For': 'Local patterns, recommendation systems',\n",
    "            'Strengths': 'Simple, no assumptions about data distribution',\n",
    "            'Weaknesses': 'Slow prediction, sensitive to irrelevant features'\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'Type': 'Ensemble',\n",
    "            'Complexity': 'High',\n",
    "            'Interpretability': 'Low',\n",
    "            'Scalability': 'Medium',\n",
    "            'Overfitting Risk': 'Medium',\n",
    "            'Best For': 'Complex patterns, competitions',\n",
    "            'Strengths': 'High performance, handles mixed data types',\n",
    "            'Weaknesses': 'Prone to overfitting, requires tuning'\n",
    "        },\n",
    "        'Neural Network': {\n",
    "            'Type': 'Neural',\n",
    "            'Complexity': 'High',\n",
    "            'Interpretability': 'Low',\n",
    "            'Scalability': 'High',\n",
    "            'Overfitting Risk': 'High',\n",
    "            'Best For': 'Complex non-linear patterns, large datasets',\n",
    "            'Strengths': 'Universal approximator, handles complex patterns',\n",
    "            'Weaknesses': 'Black box, requires large data, many hyperparameters'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(algorithm_profiles).T\n",
    "\n",
    "# Create and display algorithm profiles\n",
    "algorithm_profile_df = create_algorithm_profile()\n",
    "print(\"ALGORITHM CHARACTERISTICS PROFILE\")\n",
    "print(\"=\" * 80)\n",
    "print(algorithm_profile_df)\n",
    "\n",
    "# Create a visual comparison matrix\n",
    "numeric_characteristics = {\n",
    "    'Complexity': {'Low': 1, 'Medium': 2, 'Medium-High': 2.5, 'High': 3},\n",
    "    'Interpretability': {'Low': 1, 'Medium': 2, 'High': 3},\n",
    "    'Scalability': {'Low': 1, 'Medium': 2, 'High': 3},\n",
    "    'Overfitting Risk': {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "}\n",
    "\n",
    "# Convert to numeric for heatmap\n",
    "numeric_df = algorithm_profile_df[['Complexity', 'Interpretability', 'Scalability', 'Overfitting Risk']].copy()\n",
    "for col, mapping in numeric_characteristics.items():\n",
    "    numeric_df[col] = numeric_df[col].map(mapping)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(numeric_df, annot=True, cmap='RdYlGn', cbar_kws={'label': 'Score (1=Low, 3=High)'})\n",
    "plt.title('Algorithm Characteristics Comparison', fontsize=16, pad=20)\n",
    "plt.ylabel('Algorithms')\n",
    "plt.xlabel('Characteristics')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3147275",
   "metadata": {},
   "source": [
    "## 7. Performance vs Complexity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48cd54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance vs complexity analysis\n",
    "def create_performance_complexity_analysis():\n",
    "    \"\"\"\n",
    "    Analyze the trade-off between performance and complexity.\n",
    "    \"\"\"\n",
    "    # Get average performance scores\n",
    "    classification_perf = classification_df.groupby('Algorithm')['Accuracy'].mean()\n",
    "    regression_perf = regression_df.groupby('Algorithm')['R² Score'].mean()\n",
    "    \n",
    "    # Get training times\n",
    "    classification_time = classification_df.groupby('Algorithm')['Training Time (s)'].mean()\n",
    "    regression_time = regression_df.groupby('Algorithm')['Training Time (s)'].mean()\n",
    "    \n",
    "    # Complexity mapping\n",
    "    complexity_map = {'Low': 1, 'Medium': 2, 'Medium-High': 2.5, 'High': 3}\n",
    "    algorithm_complexity = algorithm_profile_df['Complexity'].map(complexity_map)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Classification: Performance vs Complexity\n",
    "    for alg in classification_perf.index:\n",
    "        if alg in algorithm_complexity.index:\n",
    "            axes[0].scatter(algorithm_complexity[alg], classification_perf[alg], \n",
    "                          s=100, alpha=0.7, label=alg)\n",
    "    \n",
    "    axes[0].set_xlabel('Algorithm Complexity')\n",
    "    axes[0].set_ylabel('Average Accuracy')\n",
    "    axes[0].set_title('Classification: Performance vs Complexity')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Regression: Performance vs Complexity\n",
    "    for alg in regression_perf.index:\n",
    "        # Map regression algorithms to base algorithm names\n",
    "        base_alg = alg.split(' (')[0]  # Remove kernel specification\n",
    "        if base_alg == 'Linear Regression' or base_alg == 'Ridge Regression' or base_alg == 'Lasso Regression':\n",
    "            base_alg = 'Logistic Regression'  # Use similar complexity\n",
    "        elif base_alg == 'Polynomial':\n",
    "            base_alg = 'Neural Network'  # Higher complexity\n",
    "        elif base_alg == 'SVR':\n",
    "            base_alg = 'SVM (RBF)'\n",
    "        elif base_alg == 'K-NN Regression':\n",
    "            base_alg = 'K-NN (k=5)'\n",
    "        \n",
    "        if base_alg in algorithm_complexity.index:\n",
    "            axes[1].scatter(algorithm_complexity[base_alg], regression_perf[alg], \n",
    "                          s=100, alpha=0.7, label=alg)\n",
    "    \n",
    "    axes[1].set_xlabel('Algorithm Complexity')\n",
    "    axes[1].set_ylabel('Average R² Score')\n",
    "    axes[1].set_title('Regression: Performance vs Complexity')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return classification_perf, regression_perf\n",
    "\n",
    "class_perf, reg_perf = create_performance_complexity_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e02a6e",
   "metadata": {},
   "source": [
    "## 8. Algorithm Selection Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create algorithm selection guide\n",
    "def create_selection_guide():\n",
    "    \"\"\"\n",
    "    Create a comprehensive algorithm selection guide.\n",
    "    \"\"\"\n",
    "    selection_guide = {\n",
    "        'Scenario': [\n",
    "            'Small dataset (< 1K samples)',\n",
    "            'Large dataset (> 100K samples)',\n",
    "            'High interpretability required',\n",
    "            'Maximum performance priority',\n",
    "            'Fast training required',\n",
    "            'Fast prediction required',\n",
    "            'Mixed data types',\n",
    "            'High-dimensional data',\n",
    "            'Non-linear relationships',\n",
    "            'Prone to overfitting',\n",
    "            'Text classification',\n",
    "            'Image classification',\n",
    "            'Time series prediction',\n",
    "            'Anomaly detection'\n",
    "        ],\n",
    "        'Classification Recommendation': [\n",
    "            'Naive Bayes, K-NN',\n",
    "            'Logistic Regression, Neural Network',\n",
    "            'Decision Tree, Logistic Regression',\n",
    "            'Random Forest, Gradient Boosting',\n",
    "            'Logistic Regression, Naive Bayes',\n",
    "            'Naive Bayes, Logistic Regression',\n",
    "            'Random Forest, Decision Tree',\n",
    "            'SVM, Neural Network',\n",
    "            'SVM (RBF), Random Forest',\n",
    "            'Random Forest, SVM',\n",
    "            'Naive Bayes, SVM (Linear)',\n",
    "            'Neural Network, SVM (RBF)',\n",
    "            'Random Forest, Gradient Boosting',\n",
    "            'SVM (RBF), Random Forest'\n",
    "        ],\n",
    "        'Regression Recommendation': [\n",
    "            'Linear Regression, K-NN',\n",
    "            'Linear Regression, Neural Network',\n",
    "            'Linear Regression, Decision Tree',\n",
    "            'Random Forest, Gradient Boosting',\n",
    "            'Linear Regression, Ridge',\n",
    "            'Linear Regression, K-NN',\n",
    "            'Random Forest, Decision Tree',\n",
    "            'SVR, Neural Network',\n",
    "            'SVR (RBF), Random Forest',\n",
    "            'Ridge, Lasso',\n",
    "            'N/A',\n",
    "            'Neural Network, SVR (RBF)',\n",
    "            'Random Forest, Gradient Boosting',\n",
    "            'SVR (RBF), Random Forest'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(selection_guide)\n",
    "\n",
    "# Display selection guide\n",
    "selection_df = create_selection_guide()\n",
    "print(\"ALGORITHM SELECTION GUIDE\")\n",
    "print(\"=\" * 80)\n",
    "print(selection_df.to_string(index=False))\n",
    "\n",
    "# Summary recommendations\n",
    "print(\"\\n\\nQUICK SELECTION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(\" BEST OVERALL CLASSIFICATION:\")\n",
    "top_3_class = class_perf.nlargest(3)\n",
    "for i, (alg, score) in enumerate(top_3_class.items(), 1):\n",
    "    print(f\"   {i}. {alg}: {score:.3f}\")\n",
    "\n",
    "print(\"\\n BEST OVERALL REGRESSION:\")\n",
    "top_3_reg = reg_perf.nlargest(3)\n",
    "for i, (alg, score) in enumerate(top_3_reg.items(), 1):\n",
    "    print(f\"   {i}. {alg}: {score:.3f}\")\n",
    "\n",
    "print(\"\\n FASTEST ALGORITHMS:\")\n",
    "fastest_class = classification_df.groupby('Algorithm')['Training Time (s)'].mean().nsmallest(3)\n",
    "for i, (alg, time) in enumerate(fastest_class.items(), 1):\n",
    "    print(f\"   {i}. {alg}: {time:.4f}s\")\n",
    "\n",
    "print(\"\\n MOST INTERPRETABLE:\")\n",
    "interpretable = algorithm_profile_df[algorithm_profile_df['Interpretability'] == 'High'].index.tolist()\n",
    "for i, alg in enumerate(interpretable, 1):\n",
    "    print(f\"   {i}. {alg}\")\n",
    "\n",
    "print(\"\\n GENERAL PURPOSE RECOMMENDATIONS:\")\n",
    "print(\"   • Start with: Random Forest (good balance of performance and robustness)\")\n",
    "print(\"   • For interpretability: Logistic Regression or Decision Tree\")\n",
    "print(\"   • For maximum performance: Gradient Boosting or Neural Network\")\n",
    "print(\"   • For fast prototyping: Naive Bayes or K-NN\")\n",
    "print(\"   • For high-dimensional data: SVM or Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee4ead0",
   "metadata": {},
   "source": [
    "## 9. Key Insights and Conclusions\n",
    "\n",
    "### Performance Analysis Summary\n",
    "\n",
    "Based on our comprehensive evaluation across multiple datasets, several key insights emerge:\n",
    "\n",
    "#### Classification Champions\n",
    "1. **Random Forest** consistently delivers excellent performance across all datasets\n",
    "2. **Gradient Boosting** achieves the highest peak performance but requires careful tuning\n",
    "3. **SVM** excels on high-dimensional and complex datasets\n",
    "\n",
    "#### Regression Leaders\n",
    "1. **Random Forest** again shows robust performance across different problem types\n",
    "2. **Gradient Boosting** provides the best performance for complex relationships\n",
    "3. **Linear methods** (Ridge/Lasso) work well for linear relationships and high-dimensional data\n",
    "\n",
    "#### Trade-offs Observed\n",
    "\n",
    "**Performance vs Interpretability:**\n",
    "- High interpretability: Logistic Regression, Decision Trees\n",
    "- High performance: Random Forest, Gradient Boosting, Neural Networks\n",
    "- Best balance: Random Forest (medium interpretability, high performance)\n",
    "\n",
    "**Performance vs Speed:**\n",
    "- Fastest: Naive Bayes, Logistic Regression\n",
    "- Good balance: Random Forest, K-NN\n",
    "- Slowest but powerful: SVM, Neural Networks\n",
    "\n",
    "#### Algorithm-Specific Insights\n",
    "\n",
    "- **Random Forest:** Most versatile, handles mixed data types, provides feature importance\n",
    "- **Gradient Boosting:** Best for competitions and maximum accuracy\n",
    "- **SVM:** Excellent for high-dimensional data and complex boundaries\n",
    "- **Neural Networks:** Best for very large datasets and complex patterns\n",
    "- **Logistic Regression:** Excellent baseline, fast, interpretable\n",
    "- **Decision Trees:** Most interpretable but prone to overfitting\n",
    "- **Naive Bayes:** Surprisingly effective for text and small datasets\n",
    "- **K-NN:** Simple but effective for local pattern recognition\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "1. **Always start with Random Forest** as your baseline - it's robust and performs well across various scenarios\n",
    "2. **Use cross-validation** to get reliable performance estimates\n",
    "3. **Consider the problem context** - interpretability might be more important than marginal performance gains\n",
    "4. **Experiment with ensemble methods** for production systems\n",
    "5. **Remember the no-free-lunch theorem** - no single algorithm works best for all problems\n",
    "\n",
    "### Future Considerations\n",
    "\n",
    "- Modern ensemble methods (XGBoost, LightGBM, CatBoost)\n",
    "- Deep learning for unstructured data\n",
    "- AutoML for automated algorithm selection\n",
    "- Specialized algorithms for specific domains (time series, NLP, computer vision)\n",
    "\n",
    "This analysis provides a solid foundation for algorithm selection in practical machine learning projects."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
